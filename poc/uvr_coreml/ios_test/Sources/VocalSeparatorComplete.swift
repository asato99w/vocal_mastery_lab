import Foundation
import CoreML
import Accelerate
import AVFoundation

/// å®Œå…¨ç‰ˆéŸ³æºåˆ†é›¢ã‚¨ãƒ³ã‚¸ãƒ³
///
/// AVAudioFile + STFT + CoreML ã‚’çµ±åˆã—ãŸã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰å®Ÿè£…
@available(iOS 17.0, macOS 14.0, *)
class VocalSeparatorComplete {

    // MARK: - Properties

    /// CoreMLãƒ¢ãƒ‡ãƒ«
    private let model: MLModel

    /// STFTãƒ—ãƒ­ã‚»ãƒƒã‚µ (V2: vDSP_DFT with librosa compatibility)
    private let stftProcessor: STFTProcessorV2

    /// ãƒ¢ãƒ‡ãƒ«è¨­å®š
    private let configuration: ModelConfiguration

    // MARK: - Types

    struct ModelConfiguration {
        /// FFTã‚µã‚¤ã‚º
        let fftSize: Int

        /// ãƒ›ãƒƒãƒ—ã‚µã‚¤ã‚º
        let hopSize: Int

        /// ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆ
        let sampleRate: Double

        /// ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºï¼ˆæ™‚é–“ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰
        let chunkSize: Int

        static let `default` = ModelConfiguration(
            fftSize: 4096,
            hopSize: 1024,
            sampleRate: 44100,
            chunkSize: 256
        )
    }

    struct SeparatedAudio {
        /// ãƒœãƒ¼ã‚«ãƒ«ãƒˆãƒ©ãƒƒã‚¯
        let vocals: AudioFileProcessor.AudioData

        /// ä¼´å¥ãƒˆãƒ©ãƒƒã‚¯
        let instrumental: AudioFileProcessor.AudioData
    }

    enum SeparationError: Error {
        case modelLoadFailed(String)
        case predictionFailed(String)
        case invalidAudioFormat(String)
        case processingFailed(String)
    }

    // MARK: - Initialization

    /// ã‚¤ãƒ‹ã‚·ãƒ£ãƒ©ã‚¤ã‚¶
    init(modelURL: URL, configuration: ModelConfiguration = .default) throws {
        print("ğŸ”„ VocalSeparatorCompleteåˆæœŸåŒ–ä¸­...")

        // ãƒ¢ãƒ‡ãƒ«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
        let compiledURL = try MLModel.compileModel(at: modelURL)

        // ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        let mlConfig = MLModelConfiguration()
        mlConfig.computeUnits = .all

        do {
            self.model = try MLModel(contentsOf: compiledURL, configuration: mlConfig)
        } catch {
            throw SeparationError.modelLoadFailed(
                "ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: \(error.localizedDescription)"
            )
        }

        self.configuration = configuration

        // STFTãƒ—ãƒ­ã‚»ãƒƒã‚µåˆæœŸåŒ– (V2: vDSP_DFT with librosa compatibility)
        self.stftProcessor = STFTProcessorV2(
            fftSize: configuration.fftSize,
            hopSize: configuration.hopSize
        )

        print("âœ… VocalSeparatorCompleteåˆæœŸåŒ–å®Œäº†")
        print("   FFT Size: \(configuration.fftSize)")
        print("   Hop Size: \(configuration.hopSize)")
        print("   Sample Rate: \(configuration.sampleRate) Hz")
    }

    // MARK: - Public Methods

    /// éŸ³æºåˆ†é›¢å®Ÿè¡Œï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ï¼‰
    func separate(audioURL: URL) throws -> SeparatedAudio {
        print("\n" + String(repeating: "=", count: 80))
        print("ğŸµ éŸ³æºåˆ†é›¢é–‹å§‹: \(audioURL.lastPathComponent)")
        print(String(repeating: "=", count: 80))

        // 1. ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªèª­ã¿è¾¼ã¿
        print("\nğŸ“‚ ã‚¹ãƒ†ãƒƒãƒ—1: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿")
        let audioData = try AudioFileProcessor.loadAudio(
            from: audioURL,
            targetSampleRate: configuration.sampleRate
        )

        // 2. ã‚¹ãƒ†ãƒ¬ã‚ªç¢ºèª
        let stereoAudio = AudioFileProcessor.convertToStereo(audioData)

        // 3. STFTå®Ÿè¡Œ
        print("\nğŸ”„ ã‚¹ãƒ†ãƒƒãƒ—2: STFTå®Ÿè¡Œ")
        let (leftSTFT, rightSTFT) = stftProcessor.computeSTFT(audioData: stereoAudio)
        print("   Left: \(leftSTFT.frequencyBins) bins Ã— \(leftSTFT.timeFrames) frames")
        print("   Right: \(rightSTFT.frequencyBins) bins Ã— \(rightSTFT.timeFrames) frames")

        // 4. CoreMLæ¨è«–
        print("\nğŸ¤– ã‚¹ãƒ†ãƒƒãƒ—3: CoreMLæ¨è«–å®Ÿè¡Œ")
        let (vocalMask, instrumentalMask) = try predictMasks(
            leftSTFT: leftSTFT,
            rightSTFT: rightSTFT
        )

        // 5. ãƒã‚¹ã‚¯é©ç”¨
        print("\nğŸ­ ã‚¹ãƒ†ãƒƒãƒ—4: ãƒã‚¹ã‚¯é©ç”¨")
        let vocalSpec = applyComplexMask(
            spectrogram: leftSTFT,
            mask: vocalMask
        )
        let instrumentalSpec = applyComplexMask(
            spectrogram: leftSTFT,
            mask: instrumentalMask
        )

        // 6. iSTFTå®Ÿè¡Œ
        print("\nğŸ”„ ã‚¹ãƒ†ãƒƒãƒ—5: iSTFTå®Ÿè¡Œ")
        var vocals = stftProcessor.createAudioData(
            leftMagnitude: vocalSpec.magnitude,
            leftPhase: vocalSpec.phase,
            rightMagnitude: vocalSpec.magnitude,  // ç°¡æ˜“ç‰ˆ: å·¦ã‚’å³ã«ã‚³ãƒ”ãƒ¼
            rightPhase: vocalSpec.phase,
            sampleRate: configuration.sampleRate
        )

        var instrumental = stftProcessor.createAudioData(
            leftMagnitude: instrumentalSpec.magnitude,
            leftPhase: instrumentalSpec.phase,
            rightMagnitude: instrumentalSpec.magnitude,
            rightPhase: instrumentalSpec.phase,
            sampleRate: configuration.sampleRate
        )

        // iFFTã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿®æ­£å¾Œã€gainã¯ä¸è¦ã®ã¯ãš
        // ã¾ãš gain=1.0 ã§ãƒ†ã‚¹ãƒˆ
        let gain: Float = 1.0
        if gain != 1.0 {
            vocals = applyGain(vocals, gain: gain)
            instrumental = applyGain(instrumental, gain: gain)
        }

        print("\n" + String(repeating: "=", count: 80))
        print("âœ… éŸ³æºåˆ†é›¢å®Œäº†")
        print(String(repeating: "=", count: 80))

        return SeparatedAudio(
            vocals: vocals,
            instrumental: instrumental
        )
    }

    /// éŸ³å£°ä¿å­˜
    func save(
        separatedAudio: SeparatedAudio,
        vocalsURL: URL,
        instrumentalURL: URL
    ) throws {
        print("\nğŸ’¾ åˆ†é›¢éŸ³å£°ä¿å­˜ä¸­...")

        try AudioFileProcessor.saveAudio(separatedAudio.vocals, to: vocalsURL)
        try AudioFileProcessor.saveAudio(separatedAudio.instrumental, to: instrumentalURL)

        print("âœ… ä¿å­˜å®Œäº†")
    }

    // MARK: - Private Methods

    /// CoreMLæ¨è«–å®Ÿè¡Œï¼ˆãƒãƒ£ãƒ³ã‚¯å‡¦ç†ï¼‰
    private func predictMasks(
        leftSTFT: STFTProcessorV2.SpectrogramData,
        rightSTFT: STFTProcessorV2.SpectrogramData
    ) throws -> (vocal: STFTProcessorV2.SpectrogramData, instrumental: STFTProcessorV2.SpectrogramData) {

        let timeFrames = leftSTFT.timeFrames
        let freqBins = min(leftSTFT.frequencyBins, 2048)  // ãƒ¢ãƒ‡ãƒ«æœŸå¾…å€¤
        let chunkSize = configuration.chunkSize

        let numChunks = (timeFrames + chunkSize - 1) / chunkSize
        print("   ãƒãƒ£ãƒ³ã‚¯æ•°: \(numChunks) (chunk_size=\(chunkSize))")

        var vocalMasks: [[Float]] = []
        var instrumentalMasks: [[Float]] = []

        for chunkIndex in 0..<numChunks {
            let startFrame = chunkIndex * chunkSize
            let endFrame = min((chunkIndex + 1) * chunkSize, timeFrames)

            // ãƒãƒ£ãƒ³ã‚¯æŠ½å‡º
            let chunk = extractChunk(
                leftSTFT: leftSTFT,
                rightSTFT: rightSTFT,
                startFrame: startFrame,
                endFrame: endFrame,
                targetSize: chunkSize
            )

            // æ¨è«–å®Ÿè¡Œ
            let output = try predictChunk(chunk)

            // çµæœã‚’åˆ†å‰²
            let actualSize = endFrame - startFrame
            let vocalChunk = extractChannelMask(output, channel: 0, actualSize: actualSize)
            let instrumentalChunk = extractChannelMask(output, channel: 1, actualSize: actualSize)

            vocalMasks.append(contentsOf: vocalChunk)
            instrumentalMasks.append(contentsOf: instrumentalChunk)

            if (chunkIndex + 1) % 10 == 0 {
                print("   é€²æ—: \(chunkIndex + 1)/\(numChunks)")
            }
        }

        // çµæœã‚’æ•´å½¢
        let vocalMagnitude = reshape2D(vocalMasks, frequencyBins: freqBins)
        let vocalPhase = trimPhase(leftSTFT.phase, targetBins: freqBins)  // 2048ãƒ“ãƒ³ã«èª¿æ•´

        let instrumentalMagnitude = reshape2D(instrumentalMasks, frequencyBins: freqBins)
        let instrumentalPhase = trimPhase(leftSTFT.phase, targetBins: freqBins)

        return (
            STFTProcessorV2.SpectrogramData(
                magnitude: vocalMagnitude,
                phase: vocalPhase
            ),
            STFTProcessorV2.SpectrogramData(
                magnitude: instrumentalMagnitude,
                phase: instrumentalPhase
            )
        )
    }

    /// ãƒãƒ£ãƒ³ã‚¯æŠ½å‡º
    private func extractChunk(
        leftSTFT: STFTProcessorV2.SpectrogramData,
        rightSTFT: STFTProcessorV2.SpectrogramData,
        startFrame: Int,
        endFrame: Int,
        targetSize: Int
    ) -> MLMultiArray {

        let freqBins = 2048
        let actualSize = endFrame - startFrame

        // MLMultiArrayä½œæˆ [1, 4, 2048, 256]
        let inputArray = try! MLMultiArray(
            shape: [1, 4, freqBins, targetSize] as [NSNumber],
            dataType: .float32
        )

        // ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ”ãƒ¼ï¼ˆå®Ÿéƒ¨ãƒ»è™šéƒ¨ã«åˆ†è§£ï¼‰
        for t in 0..<targetSize {
            for f in 0..<freqBins {
                let index = [0, 0, f, t] as [NSNumber]  // Left Real
                if t < actualSize && f < leftSTFT.frequencyBins {
                    inputArray[index] = NSNumber(value: leftSTFT.magnitude[f][startFrame + t])
                } else {
                    inputArray[index] = 0
                }

                // ä»–ã®ãƒãƒ£ãƒ³ãƒãƒ«ï¼ˆç°¡ç•¥åŒ–: è™šéƒ¨ã¯0ã€å³ã¯å·¦ã¨åŒã˜ï¼‰
                inputArray[[0, 1, f, t] as [NSNumber]] = 0  // Left Imag
                inputArray[[0, 2, f, t] as [NSNumber]] = inputArray[index]  // Right Real
                inputArray[[0, 3, f, t] as [NSNumber]] = 0  // Right Imag
            }
        }

        return inputArray
    }

    /// ãƒãƒ£ãƒ³ã‚¯æ¨è«–
    private func predictChunk(_ input: MLMultiArray) throws -> MLMultiArray {
        let inputProvider = try MLDictionaryFeatureProvider(dictionary: [
            "input_1": MLFeatureValue(multiArray: input)
        ])

        let output = try model.prediction(from: inputProvider)

        guard let outputArray = output.featureValue(for: "var_992")?.multiArrayValue else {
            throw SeparationError.predictionFailed("å‡ºåŠ›å–å¾—å¤±æ•—")
        }

        return outputArray
    }

    /// ãƒãƒ£ãƒ³ãƒãƒ«ãƒã‚¹ã‚¯æŠ½å‡º
    private func extractChannelMask(_ output: MLMultiArray, channel: Int, actualSize: Int) -> [[Float]] {
        var mask: [[Float]] = []

        for t in 0..<actualSize {
            var frame: [Float] = []
            for f in 0..<2048 {
                let value = output[[0, channel, f, t] as [NSNumber]].floatValue
                frame.append(value)
            }
            mask.append(frame)
        }

        // DEBUG: æœ€åˆã®ãƒãƒ£ãƒ³ã‚¯ã®æœ€åˆã®ãƒ•ãƒ¬ãƒ¼ãƒ ã§å€¤ã‚’ãƒã‚§ãƒƒã‚¯
        if mask.count > 0 && mask[0].count > 0 {
            let firstValue = mask[0][0]
            let avgValue = mask[0].reduce(0, +) / Float(mask[0].count)
            print("      DEBUG extractChannelMask: channel=\(channel), first=[0][0]=\(firstValue), avg=\(avgValue)")
        }

        return mask
    }

    /// 2Dé…åˆ—ã«æ•´å½¢
    private func reshape2D(_ flatData: [[Float]], frequencyBins: Int) -> [[Float]] {
        var result: [[Float]] = Array(repeating: [], count: frequencyBins)

        for frame in flatData {
            for (f, value) in frame.enumerated() where f < frequencyBins {
                result[f].append(value)
            }
        }

        return result
    }

    /// ä½ç›¸ãƒ‡ãƒ¼ã‚¿ã‚’æŒ‡å®šã—ãŸå‘¨æ³¢æ•°ãƒ“ãƒ³æ•°ã«èª¿æ•´
    private func trimPhase(_ phase: [[Float]], targetBins: Int) -> [[Float]] {
        let timeFrames = phase[0].count
        var trimmed: [[Float]] = Array(
            repeating: Array(repeating: 0, count: timeFrames),
            count: targetBins
        )

        let actualBins = min(phase.count, targetBins)
        for f in 0..<actualBins {
            trimmed[f] = phase[f]
        }

        return trimmed
    }

    /// ã‚²ã‚¤ãƒ³é©ç”¨
    private func applyGain(_ audioData: AudioFileProcessor.AudioData, gain: Float) -> AudioFileProcessor.AudioData {
        var amplifiedSamples: [[Float]] = []

        for channel in audioData.samples {
            var amplified = channel
            var gainValue = gain
            vDSP_vsmul(channel, 1, &gainValue, &amplified, 1, vDSP_Length(channel.count))
            amplifiedSamples.append(amplified)
        }

        return AudioFileProcessor.AudioData(
            samples: amplifiedSamples,
            sampleRate: audioData.sampleRate,
            frameCount: audioData.frameCount
        )
    }

    /// è¤‡ç´ æ•°ãƒã‚¹ã‚¯é©ç”¨
    private func applyComplexMask(
        spectrogram: STFTProcessorV2.SpectrogramData,
        mask: STFTProcessorV2.SpectrogramData
    ) -> STFTProcessorV2.SpectrogramData {

        let freqBins = min(spectrogram.frequencyBins, mask.frequencyBins)
        let timeFrames = min(spectrogram.timeFrames, mask.timeFrames)

        var maskedMagnitude: [[Float]] = Array(
            repeating: Array(repeating: 0, count: timeFrames),
            count: freqBins
        )

        var maskedPhase: [[Float]] = Array(
            repeating: Array(repeating: 0, count: timeFrames),
            count: freqBins
        )

        for f in 0..<freqBins {
            for t in 0..<timeFrames {
                maskedMagnitude[f][t] = spectrogram.magnitude[f][t] * mask.magnitude[f][t]
                maskedPhase[f][t] = spectrogram.phase[f][t]
            }
        }

        return STFTProcessorV2.SpectrogramData(
            magnitude: maskedMagnitude,
            phase: maskedPhase
        )
    }
}
