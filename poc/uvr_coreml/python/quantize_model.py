#!/usr/bin/env python3
"""
CoreML ãƒ¢ãƒ‡ãƒ«é‡å­åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

8-bité‡å­åŒ–ã‚’é©ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‚’å‰Šæ¸›ã—ã€Neural Engineæœ€é©åŒ–ã‚’å®Ÿç¾ã—ã¾ã™ã€‚
"""

import sys
import coremltools as ct
from pathlib import Path
from coremltools.optimize.coreml import (
    OpPalettizerConfig,
    OptimizationConfig,
    palettize_weights
)


def quantize_model(
    input_path: Path,
    output_path: Path,
    nbits: int = 8,
    mode: str = "kmeans",
    granularity: str = "per_channel"
):
    """
    CoreMLãƒ¢ãƒ‡ãƒ«ã®é‡å­åŒ–

    Args:
        input_path: å…¥åŠ›CoreMLãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹
        output_path: å‡ºåŠ›CoreMLãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹
        nbits: é‡å­åŒ–ãƒ“ãƒƒãƒˆæ•°ï¼ˆ4, 6, 8ï¼‰
        mode: é‡å­åŒ–ãƒ¢ãƒ¼ãƒ‰ï¼ˆkmeans, uniform, customï¼‰
        granularity: ç²’åº¦ï¼ˆper_tensor, per_channel, per_blockï¼‰
    """
    print(f"\nâš™ï¸  ãƒ¢ãƒ‡ãƒ«é‡å­åŒ–é–‹å§‹")
    print(f"   å…¥åŠ›: {input_path}")
    print(f"   å‡ºåŠ›: {output_path}")
    print(f"   ãƒ“ãƒƒãƒˆæ•°: {nbits}-bit")
    print(f"   ãƒ¢ãƒ¼ãƒ‰: {mode}")
    print(f"   ç²’åº¦: {granularity}")

    try:
        # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        print("\nğŸ“‚ CoreMLãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
        mlmodel = ct.models.MLModel(str(input_path))
        print("âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")

        # å…ƒã®ã‚µã‚¤ã‚º
        original_size = sum(
            f.stat().st_size
            for f in input_path.rglob('*')
            if f.is_file()
        )
        original_size_mb = original_size / (1024 * 1024)
        print(f"   å…ƒã®ã‚µã‚¤ã‚º: {original_size_mb:.2f} MB")

        # é‡å­åŒ–è¨­å®š
        print(f"\nğŸ”§ {nbits}-bité‡å­åŒ–è¨­å®š:")

        config = OptimizationConfig(
            global_config=OpPalettizerConfig(
                mode=mode,
                nbits=nbits,
                granularity=granularity
            )
        )

        if nbits == 8:
            print("   âœ… 8-bité‡å­åŒ–: Neural Engineæœ€é©åŒ–")
            print("   âœ… per_channelç²’åº¦: ç²¾åº¦ä¿æŒ")
        elif nbits == 4:
            print("   âš ï¸  4-bité‡å­åŒ–: per_blockç²’åº¦æ¨å¥¨")
            if granularity != "per_block":
                print("   è­¦å‘Š: 4-bitã§ã¯per_blockç²’åº¦ã‚’æ¨å¥¨")

        # é‡å­åŒ–å®Ÿè¡Œ
        print("\nâš™ï¸  é‡å­åŒ–å‡¦ç†ä¸­...")
        print("   (ã“ã®å‡¦ç†ã«ã¯æ•°åˆ†ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™)")

        compressed_model = palettize_weights(mlmodel, config=config)

        print("âœ… é‡å­åŒ–å®Œäº†")

        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        print(f"\nğŸ’¾ é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ä¿å­˜ä¸­: {output_path}")
        output_path.parent.mkdir(parents=True, exist_ok=True)
        compressed_model.save(str(output_path))

        # åœ§ç¸®å¾Œã®ã‚µã‚¤ã‚º
        compressed_size = sum(
            f.stat().st_size
            for f in output_path.rglob('*')
            if f.is_file()
        )
        compressed_size_mb = compressed_size / (1024 * 1024)

        # åœ§ç¸®ç‡è¨ˆç®—
        compression_ratio = (1 - compressed_size / original_size) * 100

        print("\nğŸ“Š é‡å­åŒ–çµæœ:")
        print(f"   å…ƒã®ã‚µã‚¤ã‚º: {original_size_mb:.2f} MB")
        print(f"   é‡å­åŒ–å¾Œ: {compressed_size_mb:.2f} MB")
        print(f"   å‰Šæ¸›ç‡: {compression_ratio:.1f}%")
        print(f"   å‰Šæ¸›é‡: {original_size_mb - compressed_size_mb:.2f} MB")

        # ç†è«–å€¤ã¨ã®æ¯”è¼ƒ
        if nbits == 8:
            expected_reduction = 75  # 8-bit = 75%å‰Šæ¸›
        elif nbits == 4:
            expected_reduction = 87.5  # 4-bit = 87.5%å‰Šæ¸›
        else:
            expected_reduction = None

        if expected_reduction:
            print(f"\n   ç†è«–å‰Šæ¸›ç‡: ~{expected_reduction}%")
            if compression_ratio >= expected_reduction * 0.9:
                print("   âœ… æœŸå¾…é€šã‚Šã®åœ§ç¸®ç‡")
            else:
                print("   âš ï¸  ç†è«–å€¤ã‚ˆã‚Šä½ã„åœ§ç¸®ç‡ï¼ˆãƒ¢ãƒ‡ãƒ«æ§‹é€ ã«ã‚ˆã‚‹ï¼‰")

        print("\nâœ… é‡å­åŒ–æˆåŠŸ!")
        return compressed_model

    except Exception as e:
        print(f"\nâŒ é‡å­åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return None


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 80)
    print("ğŸ”§ CoreML ãƒ¢ãƒ‡ãƒ«é‡å­åŒ–ãƒ„ãƒ¼ãƒ«")
    print("=" * 80)

    # ãƒ‘ã‚¹è¨­å®š
    script_dir = Path(__file__).parent
    project_root = script_dir.parent
    coreml_dir = project_root / "models" / "coreml"
    quantized_dir = project_root / "models" / "quantized"

    # CoreMLãƒ¢ãƒ‡ãƒ«æ¤œç´¢
    coreml_packages = [
        p for p in coreml_dir.glob("*.mlpackage")
        if not p.name.endswith("_quantized.mlpackage")
    ]

    if not coreml_packages:
        print("\nâŒ ã‚¨ãƒ©ãƒ¼: CoreMLãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print(f"   ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {coreml_dir}")
        print("\nå…ˆã«ãƒ¢ãƒ‡ãƒ«ã‚’å¤‰æ›ã—ã¦ãã ã•ã„:")
        print("   python python/convert_to_coreml.py")
        sys.exit(1)

    # åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’ãƒªã‚¹ãƒˆè¡¨ç¤º
    print("\nğŸ“‹ é‡å­åŒ–å¯èƒ½ãªCoreMLãƒ¢ãƒ‡ãƒ«:")
    for idx, package in enumerate(coreml_packages, 1):
        size = sum(f.stat().st_size for f in package.rglob('*') if f.is_file())
        size_mb = size / (1024*1024)
        print(f"{idx}. {package.name} ({size_mb:.2f} MB)")

    # ãƒ¢ãƒ‡ãƒ«é¸æŠ
    if len(sys.argv) > 1:
        # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§æŒ‡å®š
        model_name = sys.argv[1]
        if not model_name.endswith('.mlpackage'):
            model_name += '.mlpackage'
        input_path = coreml_dir / model_name
    else:
        # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–é¸æŠ
        choice = input(f"\né‡å­åŒ–ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ (1-{len(coreml_packages)}): ").strip()
        try:
            idx = int(choice) - 1
            if 0 <= idx < len(coreml_packages):
                input_path = coreml_packages[idx]
            else:
                print("âŒ ç„¡åŠ¹ãªé¸æŠã§ã™")
                sys.exit(1)
        except ValueError:
            print("âŒ æ•°å€¤ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            sys.exit(1)

    if not input_path.exists():
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {input_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        sys.exit(1)

    # é‡å­åŒ–è¨­å®šé¸æŠ
    print("\nâš™ï¸  é‡å­åŒ–ãƒ¬ãƒ™ãƒ«é¸æŠ:")
    print("1. 8-bit (æ¨å¥¨) - Neural Engineæœ€é©åŒ–ã€ç²¾åº¦ä¿æŒ")
    print("2. 4-bit - æœ€å¤§åœ§ç¸®ã€ç²¾åº¦ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚ã‚Š")

    quant_choice = input("é¸æŠ (1-2, ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1): ").strip() or "1"

    if quant_choice == "1":
        nbits = 8
        granularity = "per_channel"
    elif quant_choice == "2":
        nbits = 4
        granularity = "per_block"  # 4-bitã¯per_blockæ¨å¥¨
    else:
        print("âŒ ç„¡åŠ¹ãªé¸æŠã§ã™")
        sys.exit(1)

    # å‡ºåŠ›ãƒ‘ã‚¹
    output_filename = input_path.stem + f"_quantized_{nbits}bit.mlpackage"
    output_path = quantized_dir / output_filename

    # ç¢ºèª
    if output_path.exists():
        response = input(f"\næ—¢å­˜ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šæ›¸ãã—ã¾ã™ã‹? (y/N): ").strip().lower()
        if response != 'y':
            print("ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸ")
            sys.exit(0)

    # é‡å­åŒ–å®Ÿè¡Œ
    compressed_model = quantize_model(
        input_path,
        output_path,
        nbits=nbits,
        mode="kmeans",
        granularity=granularity
    )

    if compressed_model is None:
        sys.exit(1)

    print("\n" + "=" * 80)
    print(f"ğŸ“‚ é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆ: {output_path}")
    print("=" * 80)

    print("\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("   1. ãƒ†ã‚¹ãƒˆ:")
    print(f"      python python/test_conversion.py {output_filename}")
    print("   2. iOSçµ±åˆ:")
    print("      ãƒ¢ãƒ‡ãƒ«ã‚’Xcodeãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—")


if __name__ == "__main__":
    main()
